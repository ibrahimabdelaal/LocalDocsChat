{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRYSu48huSUW"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain huggingface_hub tiktoken\n",
        "!pip -q install chromadb\n",
        "!pip -q install PyPDF2 pypdf sentence_transformers\n",
        "!pip -q install -U FlagEmbedding\n",
        "!pip install InstructorEmbedding\n",
        "!pip install accelerate bitsandbyte"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTsSYoCA0O1E"
      },
      "source": [
        "* you might need to restart the kernel after installing accelerate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgfdhZ5uRpFn"
      },
      "source": [
        "## Setting up LangChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y_2-HBI3RpFn"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "XHVE9uFb3Ajj"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceInstructEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from transformers import pipeline\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.vectorstores import Chroma\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvIjaK53dP5l"
      },
      "source": [
        "## RetrievalQA with LLaMA 2-7B\n",
        "* the model can be changed to any open source model on hugging face hub\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKooyyqnTewP"
      },
      "source": [
        "# Note : changing the model will requrie changing the promot template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6zRY3sxHiz3"
      },
      "source": [
        "# Load the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcOdZvs3BPD1",
        "outputId": "aadca3f4-2586-4ef5-976b-50ab4a5d3d8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login  #sign in with your acceses token from HF to load meta mode (not requried if the model is open)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "FeNJ-CFZ07j3"
      },
      "outputs": [],
      "source": [
        "# name of hugging face llm to be used\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "w5Vt1Sju_G0I",
        "outputId": "d212b44f-61ba-4b33-cbf1-6b11477a0a29"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "import transformers\n",
        "import torch\n",
        "from transformers.tools.agents import AutoModelForCausalLM\n",
        "import os\n",
        "from langchain.llms import HuggingFaceHub\n",
        "\n",
        "\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model=AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    load_in_8bit = True,\n",
        "    device_map=\"auto\")\n",
        "\n",
        "\n",
        "task = \"text-generation\"\n",
        "llm = pipeline( task='text-generation',\n",
        "                           model=model,\n",
        "                            tokenizer=tokenizer,\n",
        "                            max_new_tokens=512)\n",
        "llm = HuggingFacePipeline(pipeline=llm)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UcQKUId3X2M"
      },
      "source": [
        "## Load multiple and process documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PRSeXXc_3Ypj"
      },
      "outputs": [],
      "source": [
        "# Load and process the text files\n",
        "#change the path to your docs directory\n",
        "docs_path='/content/new_papers/new_papers'\n",
        "loader = DirectoryLoader(docs_path, glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT6KgAIT_BtB",
        "outputId": "b683f0c6-dc27-46b0-c742-7bde135b0d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "219"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3__nT0D4Fkmg",
        "outputId": "578cfc51-bcb6-4633-d4c3-4daae5fe34af"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "992"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)\n",
        "len(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi9VSazP0RSN",
        "outputId": "c50d3aa7-b276-4118-9329-fe73deb0061c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2 Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4 Safety 20\\n4.1 Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20', metadata={'source': 'new_papers/new_papers/llama-2-paper.pdf', 'page': 1})"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "texts[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhs0C0FYASlM"
      },
      "source": [
        "## HF Instructor Embeddings\n",
        "# can choose any other open source embedding model :\n",
        " https://huggingface.co/spaces/mteb/leaderboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Emj46ATxtV9C"
      },
      "outputs": [],
      "source": [
        "#loading instruct model to use as embed model\n",
        "\n",
        "\n",
        "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
        "\n",
        "\n",
        "encode_kwargs = {'normalize_embeddings': True} # compute cosine similarity\n",
        "embedding_model='hkunlp/instructor-large'\n",
        "model_norm = HuggingFaceBgeEmbeddings(\n",
        "    model_name=embedding_model,\n",
        "    model_kwargs={'device': 'cuda'},\n",
        "    encode_kwargs=encode_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsYsIy8F4cdm"
      },
      "source": [
        "## create the DB\n",
        "\n",
        " T4 GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_eTIZwf4Dk2",
        "outputId": "bbcc4a34-3c9e-40f5-8e83-f0a50efd4d1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 40s, sys: 220 ms, total: 1min 40s\n",
            "Wall time: 1min 41s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Getting the embeddings from the docs and storing them in a directory\n",
        "# might take some time to process all the files\n",
        "persist_directory = 'db'\n",
        "embedding = model_norm\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siLXR-XT0JoI"
      },
      "source": [
        "##retriever"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jVWgPJXs1yRq"
      },
      "outputs": [],
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\":5})   # k is the number of documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "7t7_tsWzeCxf"
      },
      "outputs": [],
      "source": [
        "## Default LLaMA-2 prompt style\n",
        "B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "DEFAULT_SYSTEM_PROMPT = \"\"\"\\\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
        "\n",
        "def get_prompt(instruction, new_system_prompt=DEFAULT_SYSTEM_PROMPT ):\n",
        "    SYSTEM_PROMPT = B_SYS + new_system_prompt + E_SYS\n",
        "    prompt_template =  B_INST + SYSTEM_PROMPT + instruction + E_INST\n",
        "    return prompt_template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "J5axFVa2frOu",
        "outputId": "b77f4662-eae4-4090-d9ba-10f8e2fd5383"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\nCONTEXT:/n/n {context} Chat history:/n/n {history}\\n\\nQuestion: {question}[/INST]\""
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#this promot with memory (chat -memory)\n",
        "sys_prompt = \"\"\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \"\"\"\n",
        "\n",
        "instruction = \"\"\"CONTEXT:/n/n {context} Chat history:/n/n {history}\n",
        "\n",
        "Question: {question}\n",
        "Asisitant[/INST] \"\"\"\n",
        "get_prompt(instruction, sys_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "UuGIvWBEnK4I"
      },
      "outputs": [],
      "source": [
        "# this template without chat memory (chat history),the model will not keep track of conversation but you can ask independant questions\n",
        "template1=\"\"\" [INST]<<SYS>>\n",
        "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
        "<</SYS>>\n",
        "CONTEXT:/n/n {context}\n",
        "Question: {question}\n",
        "Asisitant[/INST] \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "DhRusEsDmkKp"
      },
      "outputs": [],
      "source": [
        "prompt1 = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\",'history'],\n",
        "    template=template1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "YRhy-5diouGJ",
        "outputId": "ef48c1b9-896a-4aaf-aca5-03aee8dfe062"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" [INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\nCONTEXT:/n/n {context} \\nQuestion: {question}\\nAsisitant[/INST] \""
            ]
          },
          "execution_count": 135,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt1.template"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ia-4OXa5IeP"
      },
      "source": [
        "## Creating a chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "1LmgdDIfsjXp"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt_template = get_prompt(instruction, sys_prompt)\n",
        "\n",
        "prompt2 = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\",'history']\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "GD4ge27NgSF6",
        "outputId": "4459457b-30ac-492c-a178-ba1cd103b24d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"[INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\n\\nCONTEXT:/n/n {context} Chat history:/n/n {history}\\n\\nQuestion: {question}[/INST]\""
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGx8XblM4shW",
        "outputId": "e23d78c4-1b3b-42fc-8429-e65b25ab7d9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'verbose': False, 'prompt': PromptTemplate(input_variables=['context', 'question'], template=\" [INST]<<SYS>>\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \\n<</SYS>>\\nCONTEXT:/n/n {context} \\nQuestion: {question}\\nAsisitant[/INST] \"), 'memory': ConversationBufferMemory(input_key='question')}\n"
          ]
        }
      ],
      "source": [
        "#from langchain.schema import prompt\n",
        "# create the chain to answer questions\n",
        "#you can change the here prompt without memory ,prompt without memory\n",
        "memory = ConversationBufferMemory(\n",
        "            memory_key=\"history\",\n",
        "            input_key=\"question\",\n",
        "            return_messages=False,\n",
        "\n",
        "        )\n",
        "chain_type_kwargs = {\"verbose\":False,\"prompt\":prompt1,\"memory\": memory }\n",
        "print(chain_type_kwargs)\n",
        "\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                                       chain_type=\"stuff\",\n",
        "                                       verbose=False,\n",
        "                                       retriever=retriever,\n",
        "                                       chain_type_kwargs=chain_type_kwargs,\n",
        "                                       return_source_documents=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "LZEo26mw8e5k"
      },
      "outputs": [],
      "source": [
        "## Cite sources\n",
        "\n",
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    lines = text.split('\\n')\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "zf7IuNupVeHN"
      },
      "outputs": [],
      "source": [
        "import torch ,gc\n",
        "with torch.no_grad():\n",
        "       gc.collect()\n",
        "       torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKfX4vX-5RFT",
        "outputId": "e12ad76e-41d9-4912-b1ff-a768ac260153"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ask a question: exit\n"
          ]
        }
      ],
      "source": [
        "# full example (the gpu migh overflow due to chat memory)\n",
        "import torch ,gc\n",
        "while True:\n",
        "    query = input(\"Ask a question: \")\n",
        "    if query.lower() == 'exit':\n",
        "\n",
        "      break\n",
        "    llm_response = qa_chain(query)\n",
        "    process_llm_response(llm_response)\n",
        "    with torch.no_grad():\n",
        "         gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPIhZWAR5n3X",
        "outputId": "db35875b-0987-4270-e53e-2a87ce096d48"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('similarity', <langchain.vectorstores.chroma.Chroma at 0x7e49506ea410>)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qa_chain.retriever.search_type , qa_chain.retriever.vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_lp0_796P_-",
        "outputId": "30fb9704-b11b-4d2e-b406-2d12996b6fa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INST]<<SYS>>\n",
            "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible using the context text provided. Your answers should only answer the question once and not have any text after the answer is done.\n",
            "\n",
            "If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. \n",
            "<</SYS>>\n",
            "\n",
            "CONTEXT:/n/n {context}/n\n",
            "\n",
            "Question: {question}[/INST]\n"
          ]
        }
      ],
      "source": [
        "print(qa_chain.combine_documents_chain.llm_chain.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65Bmvfjk9MOf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
